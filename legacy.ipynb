{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tự định nghĩa để chia dữ liệu thành train/test\n",
    "def custom_train_test_split(X, y, test_size=0.33, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_size = int(test_size * n_samples)\n",
    "    train_indices = indices[test_size:]\n",
    "    test_indices = indices[:test_size]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, data, regression_type='standard', lr=0.01, epochs=100, batch_size=32, optimizer='batch', lambda_reg=0.01):\n",
    "        # Chia dữ liệu thành train/test\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = custom_train_test_split(\n",
    "            data[:, :-1], data[:, -1], test_size=0.33, random_state=42)\n",
    "        \n",
    "        # Các tham số cơ bản\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.regression_type = regression_type\n",
    "        self.lambda_reg = lambda_reg  # Tham số điều chỉnh cho Ridge Regression\n",
    "        \n",
    "        # Chuẩn bị dữ liệu cho các công thức hồi quy\n",
    "        self._prepare_features()\n",
    "        \n",
    "        # Khởi tạo trọng số\n",
    "        self.weight = np.random.randn(self.X_train.shape[1], 1)\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.m = np.zeros_like(self.weight)\n",
    "        self.v = np.zeros_like(self.weight)\n",
    "        self.t = 0\n",
    "    \n",
    "    def _prepare_features(self):\n",
    "        \"\"\"Chuẩn bị dữ liệu đầu vào dựa trên loại hồi quy\"\"\"\n",
    "        if self.regression_type == 'standard':\n",
    "            pass\n",
    "        elif self.regression_type == 'polynomial':\n",
    "            self.X_train = np.vstack([self.X_train[:, 0]**2, self.X_train[:, 1], \n",
    "                                    self.X_train[:, 2]**2, self.X_train[:, 3]]).T\n",
    "            self.X_test = np.vstack([self.X_test[:, 0]**2, self.X_test[:, 1], \n",
    "                                   self.X_test[:, 2]**2, self.X_test[:, 3]]).T\n",
    "        elif self.regression_type == 'mixed':\n",
    "            self.X_train = np.vstack([self.X_train[:, 0] + self.X_train[:, 1], \n",
    "                                    self.X_train[:, 2]**2, self.X_train[:, 3]]).T\n",
    "            self.X_test = np.vstack([self.X_test[:, 0] + self.X_test[:, 1], \n",
    "                                   self.X_test[:, 2]**2, self.X_test[:, 3]]).T\n",
    "        elif self.regression_type == 'interaction':\n",
    "            self.X_train = np.vstack([self.X_train[:, 0] * self.X_train[:, 1], \n",
    "                                    self.X_train[:, 2]**2]).T\n",
    "            self.X_test = np.vstack([self.X_test[:, 0] * self.X_test[:, 1], \n",
    "                                   self.X_test[:, 2]**2]).T\n",
    "        else:\n",
    "            raise ValueError(\"regression_type must be 'standard', 'polynomial', 'mixed', or 'interaction'\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Dự đoán giá trị y từ X và weights\"\"\"\n",
    "        return np.dot(X, self.weight)\n",
    "    \n",
    "    def test_eval(self): \n",
    "        \"\"\"Đánh giá lỗi trung bình tuyệt đối trên tập test\"\"\"\n",
    "        y_hat = self.predict(self.X_test)\n",
    "        return np.mean(np.abs(y_hat - self.y_test.reshape(-1, 1)))\n",
    "    \n",
    "    def gradient(self, X, y, y_hat):\n",
    "        \"\"\"Tính gradient cho hàm mất mát MSE (không có regularization)\"\"\"\n",
    "        return 2 * np.dot(X.T, (y_hat - y.reshape(-1, 1))) / len(y)\n",
    "    \n",
    "    def ridge_gradient(self, X, y, y_hat):\n",
    "        \"\"\"Tính gradient cho Ridge Regression (có L2 regularization)\"\"\"\n",
    "        grad = self.gradient(X, y, y_hat) + 2 * self.lambda_reg * self.weight\n",
    "        return grad\n",
    "    \n",
    "    def batch_gradient_descent(self):\n",
    "        \"\"\"Batch Gradient Descent\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            y_hat = self.predict(self.X_train)\n",
    "            grad = self.gradient(self.X_train, self.y_train, y_hat)\n",
    "            self.weight -= self.lr * grad\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((y_hat - self.y_train.reshape(-1, 1)) ** 2)\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "    \n",
    "    def stochastic_gradient_descent(self):\n",
    "        \"\"\"Stochastic Gradient Descent\"\"\"\n",
    "        n_samples = len(self.X_train)\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            total_loss = 0\n",
    "            \n",
    "            for idx in indices:\n",
    "                X_i = self.X_train[idx:idx+1]\n",
    "                y_i = self.y_train[idx:idx+1]\n",
    "                y_hat = self.predict(X_i)\n",
    "                grad = self.gradient(X_i, y_i, y_hat)\n",
    "                self.weight -= self.lr * grad\n",
    "                total_loss += np.mean((y_hat - y_i) ** 2)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Average Loss: {total_loss/n_samples:.4f}')\n",
    "    \n",
    "    def adam_optimizer(self):\n",
    "        \"\"\"Adam Optimization\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            self.t += 1\n",
    "            y_hat = self.predict(self.X_train)\n",
    "            grad = self.gradient(self.X_train, self.y_train, y_hat)\n",
    "            \n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            self.weight -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((y_hat - self.y_train.reshape(-1, 1)) ** 2)\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "    \n",
    "    def pseudo_inverse(self):\n",
    "        \"\"\"Pseudo-inverse method\"\"\"\n",
    "        X = self.X_train\n",
    "        y = self.y_train.reshape(-1, 1)\n",
    "        self.weight = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "        loss = np.mean((self.predict(X) - y) ** 2)\n",
    "        print(f'Pseudo-inverse Loss: {loss:.4f}')\n",
    "    \n",
    "    def ridge_analytical(self):\n",
    "        \"\"\"Ridge Regression với Analytical Solution\"\"\"\n",
    "        X = self.X_train\n",
    "        y = self.y_train.reshape(-1, 1)\n",
    "        n_features = X.shape[1]\n",
    "        I = np.eye(n_features)\n",
    "        self.weight = np.linalg.inv(X.T @ X + self.lambda_reg * I) @ X.T @ y\n",
    "        loss = np.mean((self.predict(X) - y) ** 2) + self.lambda_reg * np.sum(self.weight ** 2)\n",
    "        print(f'Ridge Analytical Loss: {loss:.4f}')\n",
    "    \n",
    "    def ridge_gradient_descent(self):\n",
    "        \"\"\"Ridge Regression với Gradient Descent\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            y_hat = self.predict(self.X_train)\n",
    "            grad = self.ridge_gradient(self.X_train, self.y_train, y_hat)\n",
    "            self.weight -= self.lr * grad\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((y_hat - self.y_train.reshape(-1, 1)) ** 2) + self.lambda_reg * np.sum(self.weight ** 2)\n",
    "                print(f'Epoch {epoch}, Ridge Loss: {loss:.4f}')\n",
    "    \n",
    "    def least_squares(self):\n",
    "        \"\"\"Least Squares với np.linalg.lstsq\"\"\"\n",
    "        X = self.X_train\n",
    "        y = self.y_train.reshape(-1, 1)\n",
    "        self.weight, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)\n",
    "        loss = np.mean((self.predict(X) - y) ** 2) if residuals.size == 0 else residuals[0]\n",
    "        print(f'Least Squares Loss: {loss:.4f}')\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Huấn luyện mô hình với optimizer được chỉ định\"\"\"\n",
    "        if self.optimizer == 'batch':\n",
    "            self.batch_gradient_descent()\n",
    "        elif self.optimizer == 'sgd':\n",
    "            self.stochastic_gradient_descent()\n",
    "        elif self.optimizer == 'adam':\n",
    "            self.adam_optimizer()\n",
    "        elif self.optimizer == 'pseudo':\n",
    "            self.pseudo_inverse()\n",
    "        elif self.optimizer == 'ridge_analytical':\n",
    "            self.ridge_analytical()\n",
    "        elif self.optimizer == 'ridge_gd':\n",
    "            self.ridge_gradient_descent()\n",
    "        elif self.optimizer == 'least_squares':\n",
    "            self.least_squares()\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer must be one of: 'batch', 'sgd', 'adam', 'pseudo', 'ridge_analytical', 'ridge_gd', 'least_squares'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Linear Regression with Least Squares:\n",
      "Least Squares Loss: 0.0000\n",
      "Test Error: 0.0000\n",
      "\n",
      "Polynomial Regression with Ridge Gradient Descent:\n",
      "Epoch 0, Ridge Loss: 1709.1865\n",
      "Epoch 10, Ridge Loss: 66875953316229920293288211906560.0000\n",
      "Epoch 20, Ridge Loss: 2649573376120226973124785068083143982742290483811234705571840.0000\n",
      "Epoch 30, Ridge Loss: 104974041151222040900252131509142434569025650139747425359534151855930059494010010036338688.0000\n",
      "Epoch 40, Ridge Loss: 4158990052864434194613150512262897968897169911416808947354917558347271639030506773867310224623022526030952831255576576.0000\n",
      "Test Error: 5711401360752179090527393873408653663500458931260987569596839555607035904.0000\n",
      "\n",
      "Mixed Terms Regression with Adam:\n",
      "Epoch 0, Loss: 463.4508\n",
      "Epoch 10, Loss: 288.2138\n",
      "Epoch 20, Loss: 163.2111\n",
      "Epoch 30, Loss: 85.7249\n",
      "Epoch 40, Loss: 45.6176\n",
      "Test Error: 5.5119\n",
      "\n",
      "Interaction Terms Regression with Least Squares:\n",
      "Least Squares Loss: 0.1287\n",
      "Test Error: 0.0655\n"
     ]
    }
   ],
   "source": [
    "# Dữ liệu mẫu để kiểm tra\n",
    "np.random.seed(42)\n",
    "X = np.array([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7], [5, 6, 7, 8]])\n",
    "y = np.array([10, 15, 20, 25, 30])\n",
    "data = np.hstack((X, y.reshape(-1, 1)))\n",
    "\n",
    "# Thử nghiệm với các optimizer\n",
    "print(\"Standard Linear Regression with Least Squares:\")\n",
    "model1 = Model(data, regression_type='standard', optimizer='least_squares')\n",
    "model1.fit()\n",
    "print(f\"Test Error: {model1.test_eval():.4f}\\n\")\n",
    "\n",
    "print(\"Polynomial Regression with Ridge Gradient Descent:\")\n",
    "model2 = Model(data, regression_type='polynomial', optimizer='ridge_gd', epochs=50, lambda_reg=0.1)\n",
    "model2.fit()\n",
    "print(f\"Test Error: {model2.test_eval():.4f}\\n\")\n",
    "\n",
    "print(\"Mixed Terms Regression with Adam:\")\n",
    "model3 = Model(data, regression_type='mixed', optimizer='adam', epochs=50)\n",
    "model3.fit()\n",
    "print(f\"Test Error: {model3.test_eval():.4f}\\n\")\n",
    "\n",
    "print(\"Interaction Terms Regression with Least Squares:\")\n",
    "model4 = Model(data, regression_type='interaction', optimizer='least_squares')\n",
    "model4.fit()\n",
    "print(f\"Test Error: {model4.test_eval():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
